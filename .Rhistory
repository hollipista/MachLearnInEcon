method = 1 #  method set to 1 for Mallows model average estimates,  set to 2 for Jackknife model average estimates
# Define subset. Suppose there are 3 candidate models for the model averaging.
# Model 1: y=beta1*x1+beta2*x2+e
# Model 2: y=beta1*x1+beta3*x3+e
# Model 3: y=beta1*x1+beta2*x2+beta4*x4+e
subset <- matrix(c(1,1,1,1,0,1,0,1,0,0,0,1),3,4)
y <- as.matrix(y)
x <- as.matrix(x)
s <- as.matrix(subset)
n <- nrow(x)
p <- ncol(x)
if ((nrow(s)==1) && (ncol(s)==1)){
if (subset == 1){
s <- matrix(1,nrow=p,ncol=p)
s[upper.tri(s)] <- 0
zero <- matrix(0,nrow=1,ncol=p)
s <- rbind(zero,s)
}
if (subset == 2){
s <- matrix(0,nrow=2^p,ncol=p)
s0 <- matrix(c(1,rep(0,p-1)),1,p)
s1 <- matrix(c(rep(0,p)),1,p)
for (i in 2:2^p){
s1 <- s0 + s1
for (j in 1:p){
if (s1[1,j] == 2){
s1[1,j+1] <- s1[1,j+1]+1
s1[1,j] <- 0
}
}
s[i,] <- s1
}
}
}
m <- nrow(s)
bbeta <- matrix(0,nrow=p,ncol=m)
if (method == 2) ee <- matrix(0,nrow=n,ncol=m)
for (j in 1:m){
ss <- matrix(1,nrow=n,ncol=1) %*% s[j,]
indx1 <- which(ss[,]==1)
xs <- as.matrix(x[indx1])
xs <- matrix(xs,nrow=n,ncol=nrow(xs)/n)
if (sum(ss)==0){
xs <- x
betas <- matrix(0,nrow=p,ncol=1)
indx2 <- matrix(c(1:p),nrow=p,ncol=1)
}
if (sum(ss)>0){
betas <- solve(t(xs)%*%xs)%*%t(xs)%*%y
indx2 <- as.matrix(which(s[j,]==1))
}
beta0 <- matrix(0,nrow=p,ncol=1)
beta0[indx2] <- betas
bbeta[,j] <- beta0
if (method == 2){
ei <- y - xs %*% betas
hi <- diag(xs %*% solve(t(xs) %*% xs) %*% t(xs))
ee[,j] <- ei*(1/(1-hi))
}
}
if (method == 1){
ee <- y %*% matrix(1,nrow=1,ncol=m) - x %*% bbeta
ehat <- y - x %*% bbeta[,m]
sighat <- (t(ehat) %*% ehat)/(n-p)
}
a1 <- t(ee) %*% ee
if (qr(a1)$rank<ncol(ee)) a1 <- a1 + diag(m)*1e-10
if (method == 1) a2 <- matrix(c(-sighat*rowSums(s)),m,1)
if (method == 2) a2 <- matrix(0,nrow=m,ncol=1)
a3 <- t(rbind(matrix(1,nrow=1,ncol=m),diag(m),-diag(m)))
a4 <- rbind(1,matrix(0,nrow=m,ncol=1),matrix(-1,nrow=m,ncol=1))
w0 <- matrix(1,nrow=m,ncol=1)/m
QP <- solve.QP(a1,a2,a3,a4,1)
w <- QP$solution
w <- as.matrix(w)
w <- w*(w>0)
w <- w/sum(w0)
betahat <- bbeta %*% w
ybar <- mean(y)
yhat <- x %*% betahat
ehat <- y-yhat
r2 <- sum((yhat-ybar)^2)/sum((y-ybar)^2)
if (method == 1) cn=(t(w) %*% a1 %*% w - 2*t(a2) %*% w)/n
if (method == 2) cn=(t(w) %*% a1 %*% w)/n
list(betahat=betahat,w=w,yhat=yhat,ehat=ehat,r2=r2,cn=cn)
tree_model <- tree(Y ~ ., data = df)
ctrl <- trainControl(method = "cv", number = 10)
cv_results <- train(Y ~ ., data = df, method = "rpart", trControl = ctrl)
plot(cv_results)
summary(tree_model)
library(tree)
library(caret)
library(randomForest)
library(MAMI)
#(a). Use the rnorm() function to generate a predictor X of length n = 100, as well as a
#noise vector ε of length n = 100.
set.seed(1923)
n <- 100
X <- rnorm(n)
ksz <- rnorm(n)
#(b) Generate a response vector Y of length n = 100 according to the model
#Y = β_0 + β_1X + β_2X^2 + β_3X^2 + ε,
#where β_0, β_1, β_2, β_3 are constants of your choice.
β_0 <- 1
β_1 <- 2
β_2 <- 3
β_3 <- 4
Y <- β_0 + β_1 * X + β_2 * X^2 + β_3 * X^3 + ksz
#(c) Fit a regression tree model to the simulated data, using X,X_2,X_3, . . . ,X_10 as predictors.
#Run K-fold cross-validation and create plots of the cross-validation error as a function
#of α. Report the resulting coefficient estimates, and discuss the results obtained. Compare
#the results of the pruned tree with the unpruned tree and discuss if pruning the tree will
#improve performance.
X2 <- X^2; X3 <- X^3; X4 <- X^4; X5 <- X^5; X6 <- X^6; X7 <- X^7; X8 <- X^8; X9 <- X^9; X10 <- X^10
df <- data.frame(rep(1,n), X, X2, X3, X4, X5, X6, X7, X8, X9, X10, Y)
tree_model <- tree(Y ~ ., data = df)
ctrl <- trainControl(method = "cv", number = 10)
cv_results <- train(Y ~ ., data = df, method = "rpart", trControl = ctrl)
plot(cv_results)
summary(tree_model)
#The pruning increased the performance of model.
#(d) Fit the bagging model, using [1,X,X_2,X_3, ... ,X_10] as predictors. Compared to the
#regression tree, how well does this bagging model perform on the generated data set?
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
print(bagging_model)
tree_predicted <- predict(tree_model, newdata = df)
bagging_predicted <- predict(bagging_model, df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
tree_MSE; bagging_MSE
#Bagging model has significantly lower forecast MSE.
#(e) Fit the random forest model, using [1,X,X_2,X_3, ... ,X_10] as predictors. How well
#does the random forest perform on the generated data set compared to bagging? Use the
#importance() function to show the importance of each variable.
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
print(forest_model)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
tree_MSE; bagging_MSE; forest_MSE
# RF could not improve the model
importance(forest_model)
#(f) Fit via a boosting method, using [1,X,X_2,X_3, . . . ,X_10] as predictors. Compared to
#the random forest, how well does the boosting perform on the generated data set?
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 50000, interaction.depth = 4)
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
print(forest_model)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
tree_MSE; bagging_MSE; forest_MSE
mpty_df <- data.frame(a = rep(NA, 0),
b = rep(NA, 0),
c = rep(NA, 0),
d = rep(NA, 0))
for (x in 1:3) {
tree_model <- tree(Y ~ ., data = df)
tree_predicted <- predict(tree_model, newdata = df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
bagging_predicted <- predict(bagging_model, df)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 5000, interaction.depth = 4)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
avg_df[i, ] <- c('tree_MSE', 'bagging_MSE', 'forest_MSE', 'boost_MSE')
}
library(tree)
library(caret)
library(randomForest)
library(MAMI)
#(a). Use the rnorm() function to generate a predictor X of length n = 100, as well as a
#noise vector ε of length n = 100.
set.seed(1923)
n <- 100
X <- rnorm(n)
ksz <- rnorm(n)
#(b) Generate a response vector Y of length n = 100 according to the model
#Y = β_0 + β_1X + β_2X^2 + β_3X^2 + ε,
#where β_0, β_1, β_2, β_3 are constants of your choice.
β_0 <- 1
β_1 <- 2
β_2 <- 3
β_3 <- 4
Y <- β_0 + β_1 * X + β_2 * X^2 + β_3 * X^3 + ksz
#(c) Fit a regression tree model to the simulated data, using X,X_2,X_3, . . . ,X_10 as predictors.
#Run K-fold cross-validation and create plots of the cross-validation error as a function
#of α. Report the resulting coefficient estimates, and discuss the results obtained. Compare
#the results of the pruned tree with the unpruned tree and discuss if pruning the tree will
#improve performance.
X2 <- X^2; X3 <- X^3; X4 <- X^4; X5 <- X^5; X6 <- X^6; X7 <- X^7; X8 <- X^8; X9 <- X^9; X10 <- X^10
df <- data.frame(rep(1,n), X, X2, X3, X4, X5, X6, X7, X8, X9, X10, Y)
tree_model <- tree(Y ~ ., data = df)
ctrl <- trainControl(method = "cv", number = 10)
cv_results <- train(Y ~ ., data = df, method = "rpart", trControl = ctrl)
plot(cv_results)
summary(tree_model)
#The pruning increased the performance of model.
#(d) Fit the bagging model, using [1,X,X_2,X_3, ... ,X_10] as predictors. Compared to the
#regression tree, how well does this bagging model perform on the generated data set?
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
print(bagging_model)
tree_predicted <- predict(tree_model, newdata = df)
bagging_predicted <- predict(bagging_model, df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
tree_MSE; bagging_MSE
#Bagging model has significantly lower forecast MSE.
#(e) Fit the random forest model, using [1,X,X_2,X_3, ... ,X_10] as predictors. How well
#does the random forest perform on the generated data set compared to bagging? Use the
#importance() function to show the importance of each variable.
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
print(forest_model)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
tree_MSE; bagging_MSE; forest_MSE
# RF could not improve the model
importance(forest_model)
#(f) Fit via a boosting method, using [1,X,X_2,X_3, . . . ,X_10] as predictors. Compared to
#the random forest, how well does the boosting perform on the generated data set?
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 50000, interaction.depth = 4)
library(gbm)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 50000, interaction.depth = 4)
print(boost_model)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
tree_MSE; bagging_MSE; forest_MSE; boost_MSE
avg_df <- data.frame(tree = rep(NA, 0),
bagging = rep(NA, 0),
forest = rep(NA, 0),
boost = rep(NA, 0))
for (x in 1:3) {
tree_model <- tree(Y ~ ., data = df)
tree_predicted <- predict(tree_model, newdata = df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
bagging_predicted <- predict(bagging_model, df)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 5000, interaction.depth = 4)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
avg_df[i, ] <- c('tree_MSE', 'bagging_MSE', 'forest_MSE', 'boost_MSE')
}
tree_model <- tree(Y ~ ., data = df)
tree_predicted <- predict(tree_model, newdata = df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
bagging_predicted <- predict(bagging_model, df)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 5000, interaction.depth = 4)
View(df)
View(df)
library(tree)
library(caret)
library(randomForest)
library(gbm)
library(MAMI)
set.seed(1923)
n <- 100
X <- rnorm(n)
ksz <- rnorm(n)
β_0 <- 1
β_1 <- 2
β_2 <- 3
β_3 <- 4
Y <- β_0 + β_1 * X + β_2 * X^2 + β_3 * X^3 + ksz
X2 <- X^2; X3 <- X^3; X4 <- X^4; X5 <- X^5; X6 <- X^6; X7 <- X^7; X8 <- X^8; X9 <- X^9; X10 <- X^10
df <- data.frame(X, X2, X3, X4, X5, X6, X7, X8, X9, X10, Y)
tree_model <- tree(Y ~ ., data = df)
ctrl <- trainControl(method = "cv", number = 10)
cv_results <- train(Y ~ ., data = df, method = "rpart", trControl = ctrl)
plot(cv_results)
summary(tree_model)
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
print(bagging_model)
tree_predicted <- predict(tree_model, newdata = df)
bagging_predicted <- predict(bagging_model, df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
tree_MSE; bagging_MSE
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
print(forest_model)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
tree_MSE; bagging_MSE; forest_MSE
importance(forest_model)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 50000, interaction.depth = 4)
print(boost_model)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
tree_MSE; bagging_MSE; forest_MSE; boost_MSE
avg_df <- data.frame(tree = rep(NA, 0),
bagging = rep(NA, 0),
forest = rep(NA, 0),
boost = rep(NA, 0))
for (x in 1:3) {
tree_model <- tree(Y ~ ., data = df)
tree_predicted <- predict(tree_model, newdata = df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
bagging_predicted <- predict(bagging_model, df)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 5000, interaction.depth = 4)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
avg_df[i, ] <- c('tree_MSE', 'bagging_MSE', 'forest_MSE', 'boost_MSE')
}
for (x in 1:3) {
tree_model <- tree(Y ~ ., data = df)
tree_predicted <- predict(tree_model, newdata = df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
bagging_predicted <- predict(bagging_model, df)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 5000, interaction.depth = 4)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
avg_df[x, ] <- c('tree_MSE', 'bagging_MSE', 'forest_MSE', 'boost_MSE')
}
average_values <- colMeans(avg_df)
average_table <- data.frame(Variables = names(avg_df), Average = average_values)
average_values <- colMeans(avg_df)
average_table <- data.frame(Variables = names(avg_df), Average = average_values)
print(average_table)
View(avg_df)
for (x in 1:3) {
tree_model <- tree(Y ~ ., data = df)
tree_predicted <- predict(tree_model, newdata = df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
bagging_predicted <- predict(bagging_model, df)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 5000, interaction.depth = 4)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
avg_df[x, ] <- c(tree_MSE, bagging_MSE, forest_MSE, boost_MSE)
}
average_values <- colMeans(avg_df)
View(avg_df)
drop(avg_df)
avg_df <- data.frame(tree = rep(0, 0),
bagging = rep(0),
forest = rep(0, 0),
boost = rep(0, 0))
for (x in 1:3) {
tree_model <- tree(Y ~ ., data = df)
tree_predicted <- predict(tree_model, newdata = df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
bagging_predicted <- predict(bagging_model, df)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 5000, interaction.depth = 4)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
avg_df[x, ] <- c(tree_MSE, bagging_MSE, forest_MSE, boost_MSE)
}
average_values <- colMeans(avg_df)
avg_df <- data.frame(tree = rep(0, 0),
bagging = rep(0),
forest = rep(0, 0),
boost = rep(0, 0))
NA
avg_df <- data.frame(tree = rep(NA, 0),
bagging = rep(NA, 0),
forest = rep(NA, 0),
boost = rep(NA, 0))
View(avg_df)
for (x in 1:3) {
tree_model <- tree(Y ~ ., data = df)
tree_predicted <- predict(tree_model, newdata = df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
bagging_predicted <- predict(bagging_model, df)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 5000, interaction.depth = 4)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
avg_df[x, ] <- c(tree_MSE, bagging_MSE, forest_MSE, boost_MSE)
}
class(avg_df$tree)
average_values <- colMeans(avg_df)
average_table <- data.frame(Variables = names(avg_df), Average = average_values)
print(average_table)
#In this question, you need to generate simulated data, and will then use this data to
#perform the best selection.
library(tree)
library(caret)
library(randomForest)
library(gbm)
library(MAMI)
#(a). Use the rnorm() function to generate a predictor X of length n = 100, as well as a
#noise vector ε of length n = 100.
set.seed(1923)
n <- 100
X <- rnorm(n)
ksz <- rnorm(n)
#(b) Generate a response vector Y of length n = 100 according to the model
#Y = β_0 + β_1X + β_2X^2 + β_3X^2 + ε,
#where β_0, β_1, β_2, β_3 are constants of your choice.
β_0 <- 1
β_1 <- 2
β_2 <- 3
β_3 <- 4
Y <- β_0 + β_1 * X + β_2 * X^2 + β_3 * X^3 + ksz
#(c) Fit a regression tree model to the simulated data, using X,X_2,X_3, . . . ,X_10 as predictors.
#Run K-fold cross-validation and create plots of the cross-validation error as a function
#of α. Report the resulting coefficient estimates, and discuss the results obtained. Compare
#the results of the pruned tree with the unpruned tree and discuss if pruning the tree will
#improve performance.
X2 <- X^2; X3 <- X^3; X4 <- X^4; X5 <- X^5; X6 <- X^6; X7 <- X^7; X8 <- X^8; X9 <- X^9; X10 <- X^10
df <- data.frame(X, X2, X3, X4, X5, X6, X7, X8, X9, X10, Y)
tree_model <- tree(Y ~ ., data = df)
ctrl <- trainControl(method = "cv", number = 10)
cv_results <- train(Y ~ ., data = df, method = "rpart", trControl = ctrl)
plot(cv_results)
summary(tree_model)
#The pruning increased the performance of model.
#(d) Fit the bagging model, using [1,X,X_2,X_3, ... ,X_10] as predictors. Compared to the
#regression tree, how well does this bagging model perform on the generated data set?
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
print(bagging_model)
tree_predicted <- predict(tree_model, newdata = df)
bagging_predicted <- predict(bagging_model, df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
tree_MSE; bagging_MSE
#Bagging model has significantly lower forecast MSE.
#(e) Fit the random forest model, using [1,X,X_2,X_3, ... ,X_10] as predictors. How well
#does the random forest perform on the generated data set compared to bagging? Use the
#importance() function to show the importance of each variable.
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
print(forest_model)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
tree_MSE; bagging_MSE; forest_MSE
# RF could not improve the model
importance(forest_model)
#(f) Fit via a boosting method, using [1,X,X_2,X_3, . . . ,X_10] as predictors. Compared to
#the random forest, how well does the boosting perform on the generated data set?
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 50000, interaction.depth = 4)
print(boost_model)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
tree_MSE; bagging_MSE; forest_MSE; boost_MSE
# 50k trees grown but the result is poor.
#(g) Use the following least squares model averaging method based on regressors [1,X,X_2,X_3, . . . ,X_10]
#to fit the data: 1. Mallows model, 2. Jackknife model average estimate. Which one performs
# better?
# sorry I left it to the last minutes, I can't perform the averaging methods.
# But I do not fully understand you code and I could not use any packaged for this
# aim. If you have time, please send me a sparse/short solution to averaging the
# hour above models. Thanks.
#(h) Repeat (a)-(g) 100 times via a loop. Calculate and report the averaged MSEs for all
#methods. Rank the fitting ability based on your simulation results.
avg_df <- data.frame(tree = rep(NA, 0),
bagging = rep(NA, 0),
forest = rep(NA, 0),
boost = rep(NA, 0))
for (x in 1:100) {
tree_model <- tree(Y ~ ., data = df)
tree_predicted <- predict(tree_model, newdata = df)
tree_MSE <- mean((df$Y - tree_predicted)^2)
bagging_model <- randomForest(Y ~ ., data = df, mtry = 10, ntree = 500)
bagging_predicted <- predict(bagging_model, df)
bagging_MSE <- mean((df$Y - bagging_predicted)^2)
forest_model <- randomForest(Y ~ ., data = df, ntree = 500)
forest_predicted <- predict(forest_model, df)
forest_MSE <- mean((df$Y - forest_predicted)^2)
boost_model <- gbm(Y ~ ., data = df, distribution = "gaussian",
n.trees = 5000, interaction.depth = 4)
boost_predicted <- predict(boost_model, df)
boost_MSE <- mean((df$Y - boost_predicted)^2)
avg_df[x, ] <- c(tree_MSE, bagging_MSE, forest_MSE, boost_MSE)
}
average_values <- colMeans(avg_df)
average_table <- data.frame(Variables = names(avg_df), Average = average_values)
print(average_table)
