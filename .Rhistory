ungroup()
}
# here I calculate the change of current year on last 3 years rolling avg
for (col in colnames) { #change variables
new_col_name <- paste0(col, "_chg")
roll_col_name <- paste0(col, "_rollmean")
df <- df %>%
group_by(company_name) %>%
mutate(!!new_col_name := (!!sym(col)-lag(!!sym(roll_col_name), n=1))/abs(lag(!!sym(roll_col_name), n=1))) %>%
ungroup()
}
df <- df %>% # I keep years that has 4 year lead: 3 for the moving average + 1 for the change
drop_na()
table(df$bankrupt) # number of bankrupcies
colnameschg <- c("X1_chg", "X2_chg", "X3_chg", "X4_chg", "X5_chg", "X6_chg", "X7_chg", "X8_chg", "X9_chg",
"X10_chg", "X11_chg", "X12_chg", "X13_chg", "X14_chg", "X15_chg", "X16_chg", "X17_chg", "X18_chg")
describe(df)
df %>%
summarise(across(where(is.numeric), ~ quantile(., probs = 0.995, na.rm = TRUE))) %>%
t()
# I'm winsorizing the extreme increases at +300%
df <- df %>%
mutate(across(all_of(colnameschg), ~ pmin(3, .)))
df <- df %>%
select(all_of(c(colnameschg, "bankrupt")))
# set train and test sets (70% train / 30% test)
set.seed(1923)
train <- sample(1:nrow(df), nrow(df) * 0.7)
train_data <- df[train, ]
test_data <- df[-train, ]
# Due to the very few positive tag (=bankrupts) I've used an overweigt for the bankrupt=1 cases
weights <- ifelse(train_data$bankrupt == 1, 15, 1)
tree_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class")
print(tree_model)
# show tree structure
plot(tree_model)
text(tree_model)
# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)
# prune the tree
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)
# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)
# set train and test sets (70% train / 30% test)
set.seed(1923)
train <- sample(1:nrow(df), nrow(df) * 0.7)
train_data <- df[train, ]
test_data <- df[-train, ]
# Due to the very few positive tag (=bankrupts) I've used an overweigt for the bankrupt=1 cases
weights <- ifelse(train_data$bankrupt == 1, 25, 1)
tree_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class")
print(tree_model)
# show tree structure
plot(tree_model)
text(tree_model)
# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)
# prune the tree
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)
# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)
# set train and test sets (70% train / 30% test)
set.seed(1923)
train <- sample(1:nrow(df), nrow(df) * 0.7)
train_data <- df[train, ]
test_data <- df[-train, ]
# Due to the very few positive tag (=bankrupts) I've used an overweigt for the bankrupt=1 cases
weights <- ifelse(train_data$bankrupt == 1, 20, 1)
tree_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class")
print(tree_model)
# show tree structure
plot(tree_model)
text(tree_model)
# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)
# prune the tree
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)
# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)
# set train and test sets (70% train / 30% test)
set.seed(1923)
train <- sample(1:nrow(df), nrow(df) * 0.7)
train_data <- df[train, ]
test_data <- df[-train, ]
# Due to the very few positive tag (=bankrupts) I've used an overweigt for the bankrupt=1 cases
weights <- ifelse(train_data$bankrupt == 1, 25, 1)
tree_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class")
print(tree_model)
# show tree structure
plot(tree_model)
text(tree_model)
# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)
# prune the tree
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)
# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)
# set train and test sets (70% train / 30% test)
set.seed(1923)
train <- sample(1:nrow(df), nrow(df) * 0.7)
train_data <- df[train, ]
test_data <- df[-train, ]
# Due to the very few positive tag (=bankrupts) I've used an overweigt for the bankrupt=1 cases
weights <- ifelse(train_data$bankrupt == 1, 20, 1)
tree_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class")
print(tree_model)
# show tree structure
plot(tree_model)
text(tree_model)
# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)
# prune the tree
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)
# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)
train_data$bankrupt <- as.factor(train_data$bankrupt)
bag_model <- randomForest(bankrupt ~ ., data = train_data, mtry = 18,
weights = weights, importance = TRUE)
print(bag_model)
# mtry = 18 indicates that all 18 predictors should be considered for each split of the tree
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
weights
# set train and test sets (70% train / 30% test)
set.seed(1923)
train <- sample(1:nrow(df), nrow(df) * 0.7)
train_data <- df[train, ]
test_data <- df[-train, ]
train_data$bankrupt <- as.factor(train_data$bankrupt)
# Due to the very few positive tag (=bankrupts) I've used an overweigt for the bankrupt=1 cases
weights <- ifelse(train_data$bankrupt == 1, 20, 1)
tree_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class")
print(tree_model)
# show tree structure
plot(tree_model)
text(tree_model)
# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)
# prune the tree
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)
# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)
bag_model <- randomForest(bankrupt ~ ., data = train_data, mtry = 18,
weights = weights, importance = FALSE)
print(bag_model)
# mtry = 18 indicates that all 18 predictors should be considered for each split of the tree
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
bag_model <- randomForest(bankrupt ~ ., data = train_data, mtry = 18,
weights = weights, importance = TRUE)
print(bag_model)
# mtry = 18 indicates that all 18 predictors should be considered for each split of the tree
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
boost_model <- gbm(bankrupt ~ ., data = train_data,
weights = weights, distribution = "bernoulli",
n.trees = 5000, interaction.depth = 4)
print(boost_model)
predictions <- as.factor(predict(rf_model , newdata = test_data, n.trees = 5000))
confusion_matrix_boost<- confusionMatrix(predictions, actual_values)
print(confusion_matrix_boost)
test_data
boost_model <- gbm(bankrupt ~ ., data = train_data,
weights = weights, distribution = "bernoulli",
n.trees = 5000, interaction.depth = 4)
print(boost_model)
predictions <- as.factor(predict(boost_model , newdata = test_data, n.trees = 5000))
confusion_matrix_boost<- confusionMatrix(predictions, actual_values)
predictions
print(boost_model)
train_data
test_data
predict(boost_model , newdata = test_data, n.trees = 5000)
predictions <- as.factor(predict(boost_model , newdata = test_data))
predictions
boost_model <- gbm(bankrupt ~ ., data = train_data,
distribution = "bernoulli",
n.trees = 5000, interaction.depth = 4)
print(boost_model)
predictions <- as.factor(predict(boost_model , newdata = test_data))
predictions
train_data$bankrupt
classwt = c("0"=1, "1"=20)
classwt
bag_model <- randomForest(bankrupt ~ ., data = train_data, mtry = 18,
classwt = c("0"=1, "1"=20), importance = TRUE)
print(bag_model)
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
library(ROSE)
install.packages("ROSE")
library(ROSE)
over <- ovun.sample(bankrupt~., data = train_data, method = "over", N = nrow(train_data))$data
over
table(over$bankrupt)
table(train_data$bankrupt)
over <- ovun.sample(bankrupt~., data = train_data, method = "over", N = 1000)$data
table(train_data$bankrupt)
table(over$bankrupt)
over <- ovun.sample(bankrupt~., data = train_data, method = "over", N = 1000)$data
table(over$bankrupt)
over <- ovun.sample(bankrupt~., data = train_data, method = "over", N = 10000)$data
table(over$bankrupt)
over <- ovun.sample(bankrupt~., data = train_data, method = "over", N = 100000)$data
table(over$bankrupt)
over <- ovun.sample(bankrupt~., data = train_data, method = "over", N = 27750)$data
table(over$bankrupt)
over <- ovun.sample(bankrupt~., data = train_data, method = "over", N = 27751)$data
table(over$bankrupt)
sum(train_data$bankrupt == 0)
over <- ovun.sample(bankrupt~., data = train_data, method = "over", N = sum(train_data$bankrupt == 0)*2)$data
table(over$bankrupt)
tree_model <- rpart(bankrupt ~ ., data = over, method = "class")
print(tree_model)
plot(tree_model)
text(tree_model)
# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)
# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)
# set train and test sets (70% train / 30% test)
set.seed(1923)
train <- sample(1:nrow(df), nrow(df) * 0.7)
train_data <- df[train, ]
test_data <- df[-train, ]
train_data$bankrupt <- as.factor(train_data$bankrupt)
# Due to the very few positive tag (=bankrupts) I've used an overweigt for the bankrupt=1 cases
weights <- ifelse(train_data$bankrupt == 1, 20, 1)
tree_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class")
print(tree_model)
# show tree structure
plot(tree_model)
text(tree_model)
# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)
# prune the tree
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)
# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)
bag_model <- randomForest(bankrupt ~ ., data = over, mtry = 18,
importance = TRUE)
print(bag_model)
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
# By default, randomForest() uses p/3 variables when building a random forest of regression trees
# The random forest function inputs are the same as bagging, the difference is the called output
# We use mtry = 6.
rf_model <- randomForest(bankrupt ~ ., data = over,
weights = weights, importance = TRUE)
# By default, randomForest() uses p/3 variables when building a random forest of regression trees
# The random forest function inputs are the same as bagging, the difference is the called output
# We use mtry = 6.
rf_model <- randomForest(bankrupt ~ ., data = over,
importance = TRUE)
print(rf_model)
predictions <- as.factor(predict(rf_model , newdata = test_data))
confusion_matrix_rf <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_rf)
# set train and test sets (70% train / 30% test)
set.seed(1923)
train <- sample(1:nrow(df), nrow(df) * 0.7)
train_data <- df[train, ]
test_data <- df[-train, ]
train_data$bankrupt <- as.factor(train_data$bankrupt)
# Due to the very few positive tag (=bankrupts) I've used an overweigt for the bankrupt=1 cases
weights <- ifelse(train_data$bankrupt == 1, 20, 1)
# Here tried to use oversampling with ROSE library
# First made a balanced sample with 50% bankrupt flag then run the models on
# this dataset. The results was not better hence I went back to the original
# approach.
#over <- ovun.sample(bankrupt~., data = train_data, method = "over", N = sum(train_data$bankrupt == 0)*2)$data
#table(over$bankrupt)
tree_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class")
print(tree_model)
# show tree structure
plot(tree_model)
text(tree_model)
# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)
# prune the tree
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)
# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)
plotcp(cv_model)
plot(tree_model)
text(tree_model)
bag_model <- randomForest(bankrupt ~ ., data = train_data, weights = weights,
mtry = 18)
print(bag_model)
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
classwt = c("0"=1, "1"=20)
classwt
class(train_data$bankrupt)
train_data$bankrupt == "0"
train_data$bankrupt == "1"
bag_model <- randomForest(bankrupt ~ ., data = train_data, weights = weights,
mtry = 18,
classwt = c(1,20), importance = TRUE)
print(bag_model)
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
bag_model <- randomForest(bankrupt ~ ., data = train_data, weights = weights,
mtry = 18,
classwt = c("0" = 1, "1" = 200), importance = TRUE)
print(bag_model)
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
bag_model <- randomForest(bankrupt ~ ., data = train_data, weights = weights,
mtry = 18,
classwt = c(0 = 1, 1 = 20), importance = TRUE)
print(bag_model)
bag_model <- randomForest(bankrupt ~ ., data = train_data, weights = weights,
mtry = 18,
classwt = c(1, 20), importance = TRUE)
print(bag_model)
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
bag_model <- randomForest(bankrupt ~ ., data = train_data, weights = weights,
mtry = 18,
sampsize=c(10000,27750), importance = TRUE)
bag_model <- randomForest(bankrupt ~ ., data = train_data, weights = weights,
mtry = 18,
weights = weights, importance = TRUE)
print(bag_model)
bag_model <- randomForest(bankrupt ~ ., data = train_data,
mtry = 18,
classwt = c(1,20), importance = TRUE)
print(bag_model)
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
bag_model <- randomForest(bankrupt ~ ., data = train_data,
mtry = 18,
classwt = c(1,200), importance = TRUE)
print(bag_model)
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
bag_model <- randomForest(bankrupt ~ ., data = train_data,
mtry = 18, weights = weights,
importance = TRUE)
print(bag_model)
# mtry = 18 indicates that all 18 predictors should be considered for each split of the tree
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
# By default, randomForest() uses p/3 variables when building a random forest of regression trees
# The random forest function inputs are the same as bagging, the difference is the called output
# We use mtry = 6.
rf_model <- randomForest(bankrupt ~ ., data = train_data,
weights = weights, importance = TRUE)
print(rf_model)
predictions <- as.factor(predict(rf_model , newdata = test_data))
confusion_matrix_rf <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_rf)
# Due to the very few positive tag (=bankrupts) I've used an overweigt for the bankrupt=1 cases
weights <- ifelse(train_data$bankrupt == "1", 20, 1)
weights
bag_model <- randomForest(bankrupt ~ ., data = train_data,
mtry = 18, weights = weights,
importance = TRUE)
print(bag_model)
# mtry = 18 indicates that all 18 predictors should be considered for each split of the tree
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
table(train_data$bankrupt)[1])
table(train_data$bankrupt)[1]
train_data$bankrupt
table(train_data$bankrupt)
table(train_data$bankrupt)[1]
table(train_data$bankrupt)[2]
weights <- ifelse(train_data$bankrupt == "1",
(1/table(train_data$bankrupt)[1]) * 0.5,
(1/table(train_data$bankrupt)[2]) * 0.5)
weights
weights <- ifelse(train_data$bankrupt == "0",
(1/table(train_data$bankrupt)[1]) * 0.5,
(1/table(train_data$bankrupt)[2]) * 0.5)
weights
tree_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class")
print(tree_model)
# show tree structure
plot(tree_model)
text(tree_model)
# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)
# prune the tree
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)
# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)
bag_model <- randomForest(bankrupt ~ ., data = train_data,
mtry = 18, weights = weights,
importance = TRUE)
print(bag_model)
# mtry = 18 indicates that all 18 predictors should be considered for each split of the tree
predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
# By default, randomForest() uses p/3 variables when building a random forest of regression trees
# The random forest function inputs are the same as bagging, the difference is the called output
# We use mtry = 6.
rf_model <- randomForest(bankrupt ~ ., data = train_data,
weights = weights, importance = TRUE)
print(rf_model)
predictions <- as.factor(predict(rf_model , newdata = test_data))
confusion_matrix_rf <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_rf)
boost_model <- gbm(bankrupt ~ ., data = train_data, weights = weights,
distribution = "bernoulli",
n.trees = 5000, interaction.depth = 4)
print(boost_model)
predictions <- as.factor(predict(boost_model , newdata = test_data))
confusion_matrix_boost<- confusionMatrix(predictions, actual_values)
predictions
class(boost_model)
class(test_data)
predictions <- as.factor(predict(boost_model , newdata = as.dataframe(test_data)))
predictions <- as.factor(predict(boost_model , newdata = as.data.frame(test_data)))
confusion_matrix_boost<- confusionMatrix(predictions, actual_values)
predictions
test_data
train_data
predictions <- as.factor(predict(boost_model , newdata =train_data))
confusion_matrix_boost<- confusionMatrix(predictions, actual_values)
boost_model <- gbm(bankrupt ~ ., data = train_data, distribution = "bernoulli", n.trees = 5000, interaction.depth = 4)
# Predikció a teszt adathalmazon
# Tegyük fel, hogy van egy 'test_data' adatkeretünk, amely tartalmazza a teszt adatokat.
predictions <- predict(boost_model, newdata = test_data, n.trees = 5000, type = "response")  # 'type = "response"' a bináris predikcióhoz
confusion_matrix_boost <- confusionMatrix(ifelse(predictions > 0.5, 1, 0), actual_values)
predictions
