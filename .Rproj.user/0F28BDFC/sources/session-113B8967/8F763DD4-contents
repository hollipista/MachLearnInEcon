---
title: "Machine Learning in Econometrics - Group Work"
author: "Hollosi, Pokasz, Soos, Szabo"
date: "2023-12-12"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(zoo)
library(Hmisc)

#library(tree) 
library(rpart)
#library(ISLR2)
library(caret)
library(randomForest)
library(gbm)
library(ROSE)
```

## About dataset

A novel dataset for bankruptcy prediction related to American public companies listed on the New York Stock Exchange and NASDAQ is provided. The dataset comprises accounting data from 8,262 distinct companies recorded during the period spanning from 1999 to 2018.

For further information: [kaggle.com](https://www.kaggle.com/datasets/utkarshx27/american-companies-bankruptcy-prediction-dataset/)

Status_label column contains the flag whether the company has gone to bankrupt after the last reported year.
It's permanently 'failed' for seased companies not just for the last period!

```{r data intake}

download.file(url = "https://raw.githubusercontent.com/hollipista/MachLearnInEcon/main/american_bankruptcy.zip", 
              destfile = "american_bankruptcy.zip", mode = "wb")
unzip("american_bankruptcy.zip")

df <- read_csv("american_bankruptcy.csv") %>% 
  arrange(company_name, year) %>% 
  group_by(company_name) %>%
  mutate(last = ifelse(row_number() == max(row_number()), 1, 0)) %>% #find last reported year for each company
  ungroup() 

colnames <- c("X1", "X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", 
              "X10", "X11", "X12", "X13", "X14", "X15", "X16", "X17", "X18") 

for (col in colnames) { #moving avg for each variable
  new_col_name <- paste0(col, "_rollmean")
  df <- df %>%
    group_by(company_name) %>%
    mutate(!!new_col_name := rollmean(!!sym(col), k = 3, align = "right", fill = NA)) %>%
    ungroup()
}

df <- df %>%  #flag the last year of bankrupted companies = year before bankrupcy
  mutate(bankrupt = ifelse(last == 1 & status_label == 'failed', 1, 0))

table(df$bankrupt) # number of bankrupcies

# here I calculate the change of current year on last 3 years rolling avg
for (col in colnames) { #change variables
  new_col_name <- paste0(col, "_chg")
  roll_col_name <- paste0(col, "_rollmean")
  df <- df %>%
    group_by(company_name) %>%
    mutate(!!new_col_name := (!!sym(col)-lag(!!sym(roll_col_name), n=1))/abs(lag(!!sym(roll_col_name), n=1))) %>%
    ungroup()
}

df <- df %>% # I keep years that has 4 year lead: 3 for the moving average + 1 for the change
  drop_na()

table(df$bankrupt) # number of bankrupcies

colnameschg <- c("X1_chg", "X2_chg", "X3_chg", "X4_chg", "X5_chg", "X6_chg", "X7_chg", "X8_chg", "X9_chg", 
              "X10_chg", "X11_chg", "X12_chg", "X13_chg", "X14_chg", "X15_chg", "X16_chg", "X17_chg", "X18_chg") 
describe(df)

# There are a couple of extreme high values especially in case of X5 and X11:
# inventory and long-term debt. As both variables could be zero and the division
# by zero could result infinite floating. 

df %>% 
  summarise(across(where(is.numeric), ~ quantile(., probs = 0.995, na.rm = TRUE))) %>% 
  t()

# I'm winsorizing the extreme increases at +300%
df <- df %>%
  mutate(across(all_of(colnameschg), ~ pmin(3, .)))

df <- df %>% 
  select(all_of(c(colnameschg, "bankrupt")))
  
```

## Prepared dataset

1. I've calculated 3-month moving average for all variables
2. Get the yearly change for all: (t2-t1)/t1
3. Winsorized the extreme values (because some statments could be zero)
4. Dropped unnecessary variables

Final structure: 
bankrupt dummy: 0=no, 1=yes (at the last year before bankrupcy)
X1_chg to X18_chg: yearly change in item of financial statments (used 3-years moving average)

```{r regression tree}
# set train and test sets (70% train / 30% test)
set.seed(1923)
train <- sample(1:nrow(df), nrow(df) * 0.7)
train_data <- df[train, ]
test_data <- df[-train, ]
train_data$bankrupt <- as.factor(train_data$bankrupt)

# Due to the very few positive tag (=bankrupts) I've used an overweigt for the bankrupt=1 cases
weights <- ifelse(train_data$bankrupt == "0",
                        (1/table(train_data$bankrupt)[1]) * 0.5,
                        (1/table(train_data$bankrupt)[2]) * 0.5)

# Here tried to use oversampling with ROSE library
# First made a balanced sample with 50% bankrupt flag then run the models on
# this dataset. The results was not better hence I went back to the original
# approach.
#over <- ovun.sample(bankrupt~., data = train_data, method = "over", N = sum(train_data$bankrupt == 0)*2)$data
#table(over$bankrupt)

tree_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class")
print(tree_model)

# show tree structure
plot(tree_model)
text(tree_model)

# use the rpart.control function to see whether pruning the tree will improve performance.
cv_model <- rpart(bankrupt ~ ., data = train_data, weights = weights, method = "class",
                    control = rpart.control(cp = 0.01, minsplit = 10, xval = 10))
plotcp(cv_model)

# prune the tree
pruned_model <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
plot(pruned_model)
text(pruned_model)

# prediction
predictions <- predict(pruned_model, newdata = test_data, type = "class")
actual_values <- as.factor(test_data$bankrupt)
confusion_matrix_tree <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_tree)

```

```{r bagging}
bag_model <- randomForest(bankrupt ~ ., data = train_data, 
                          mtry = 18, weights = weights,
                          importance = TRUE)
print(bag_model)
# mtry = 18 indicates that all 18 predictors should be considered for each split of the tree

predictions <- as.factor(predict(bag_model , newdata = test_data))
confusion_matrix_bagging <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_bagging)
```

```{r random forest}
# By default, randomForest() uses p/3 variables when building a random forest of regression trees
# The random forest function inputs are the same as bagging, the difference is the called output
# We use mtry = 6.
rf_model <- randomForest(bankrupt ~ ., data = train_data, 
                         weights = weights, importance = TRUE)
print(rf_model)

predictions <- as.factor(predict(rf_model , newdata = test_data))
confusion_matrix_rf <- confusionMatrix(predictions, actual_values)
print(confusion_matrix_rf)
```

```{r boosting}
boost_model <- gbm(as.numeric("1"==bankrupt) ~ ., data = train_data, weights = weights,
                   distribution = "bernoulli", 
                   n.trees = 5000, interaction.depth = 4)
print(boost_model)

predictions <- as.factor(predict(boost_model , newdata =test_data, n.trees = 5000, type = "response"))
binary_predictions <- ifelse(as.numeric(as.character(predictions)) > 0.5, 1, 0)
sum(binary_predictions)

confusion_matrix_boost<- confusionMatrix(as.factor(binary_predictions), actual_values)
print(confusion_matrix_boost)


# threshold interations to find the optimal cut-off
sensitivity_values <- c()
specificity_values <- c()

for (iter in seq(0, 1, by = 0.05)) {
  predictions <- as.factor(ifelse(predict(boost_model, newdata = test_data, 
                                          n.trees = 5000, type = "response") > iter, 1, 0))
  confusion_matrix_boost <- confusionMatrix(predictions, actual_values)
  sensitivity_values <- c(sensitivity_values, confusion_matrix_boost$byClass["Sensitivity"])
  specificity_values <- c(specificity_values, confusion_matrix_boost$byClass["Specificity"])
}

plot(seq(0, 1, by = 0.05), sensitivity_values, type = "l", col = "blue", ylim = c(0, 1),
     xlab = "Threshold", ylab = "Érték", main = "Specificity and sensitivity as function of threshold")
lines(seq(0, 1, by = 0.05), specificity_values, type = "l", col = "red")
legend("topright", legend = c("Sensitivity", "Specificity"), col = c("blue", "red"), lty = 1)

specificity_values
sensitivity_values
```
